# Attention-based MineCLIP configuration

# Model architecture
arch: "vit_base_p16_fz.v2.t2"
hidden_dim: 512
image_feature_dim: 512
mlp_adapter_spec: "v0-2.t0"
pool_type: "attn.d2.nh8.glusw"
resolution: [160, 256]

# Attention settings
num_heads: 8
attention_dropout: 0.1
layer_dropout: 0.1
